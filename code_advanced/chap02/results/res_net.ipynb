{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Nvk-AIAraOF"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input\n",
        "from tensorflow.keras.layers import Flatten, add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw57JH5praOI",
        "outputId": "abd31c34-a645-49cb-fc34-55aca302c043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 7s 0us/step\n",
            "170508288/170498071 [==============================] - 7s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ]
        }
      ],
      "source": [
        "# training parameters\n",
        "batch_size = 32 # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "# subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameter\n",
        "n = 3\n",
        "\n",
        "# model version\n",
        "# orig paper: version = 1 (ResNet v1), \n",
        "# improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# if subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F82senq8raOJ"
      },
      "outputs": [],
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "    Arguments:\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "    Returns:\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved\n",
        "    (downsampled) by a convolutional layer with strides=2, while \n",
        "    the number of filters is doubled. Within each stage, \n",
        "    the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "    Arguments:\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "    Returns:\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, in [a])')\n",
        "    # start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            # first layer but not first stack\n",
        "            if stack > 0 and res_block == 0:  \n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            # first layer but not first stack\n",
        "            if stack > 0 and res_block == 0:\n",
        "                # linear projection residual shortcut\n",
        "                # connection to match changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or \n",
        "    also known as bottleneck layer.\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, \n",
        "    the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, \n",
        "    while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have \n",
        "    the same number filters and the same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "    Arguments:\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "    Returns:\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 110 in [b])')\n",
        "    # start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU\n",
        "    # on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                # first layer and first stage\n",
        "                if res_block == 0:  \n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                # first layer but not first stage\n",
        "                if res_block == 0:\n",
        "                    # downsample\n",
        "                    strides = 2 \n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection\n",
        "                # to match changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqTq-i6IraOL",
        "outputId": "bf9cdbda-2824-4413-e926-cc01e381c949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  0.001\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
            "                                                                  'batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 32, 32, 16)   0           ['activation_2[0][0]',           \n",
            "                                                                  'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 32, 32, 16)   0           ['activation_4[0][0]',           \n",
            "                                                                  'batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 16, 16, 32)   4640        ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 16, 16, 32)   9248        ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 16, 32)   544         ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 16, 16, 32)   0           ['conv2d_9[0][0]',               \n",
            "                                                                  'batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 16, 16, 32)   0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_10[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 16, 16, 32)   0           ['activation_8[0][0]',           \n",
            "                                                                  'batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 16, 16, 32)   0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
            "                                                                                                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 16, 16, 32)   0           ['activation_10[0][0]',          \n",
            "                                                                  'batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 32)   0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 8, 8, 64)    256         ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 8, 8, 64)    256         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 8, 8, 64)     0           ['conv2d_16[0][0]',              \n",
            "                                                                  'batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 8, 8, 64)     0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 8, 8, 64)    256         ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 8, 8, 64)     0           ['activation_14[0][0]',          \n",
            "                                                                  'batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 8, 8, 64)     0           ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 8, 8, 64)     0           ['activation_16[0][0]',          \n",
            "                                                                  'batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 8, 8, 64)     0           ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 1, 1, 64)    0           ['activation_18[0][0]']          \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 64)           0           ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           650         ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ]
        }
      ],
      "source": [
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "# enable this if pydot can be installed\n",
        "# pip install pydot\n",
        "#plot_model(model, to_file=\"%s.png\" % model_type, show_shapes=True)\n",
        "print(model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S4uZtvGqraOM"
      },
      "outputs": [],
      "source": [
        "# prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBnpajHNraON",
        "outputId": "a0f38891-2419-4357-a319-20cfdc01e1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using real-time data augmentation.\n",
            "Learning rate:  0.001\n",
            "Epoch 1/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.5949 - acc: 0.4820\n",
            "Epoch 1: val_acc improved from -inf to 0.43570, saving model to /content/saved_models/cifar10_ResNet20v1_model.001.h5\n",
            "1563/1563 [==============================] - 54s 26ms/step - loss: 1.5945 - acc: 0.4822 - val_loss: 1.9890 - val_acc: 0.4357 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 2/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.1810 - acc: 0.6361\n",
            "Epoch 2: val_acc improved from 0.43570 to 0.59290, saving model to /content/saved_models/cifar10_ResNet20v1_model.002.h5\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.1808 - acc: 0.6363 - val_loss: 1.4083 - val_acc: 0.5929 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 3/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.0215 - acc: 0.6979\n",
            "Epoch 3: val_acc did not improve from 0.59290\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 1.0215 - acc: 0.6979 - val_loss: 1.6993 - val_acc: 0.5529 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 4/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.9319 - acc: 0.7310\n",
            "Epoch 4: val_acc improved from 0.59290 to 0.72040, saving model to /content/saved_models/cifar10_ResNet20v1_model.004.h5\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.9319 - acc: 0.7310 - val_loss: 0.9842 - val_acc: 0.7204 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 5/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.8721 - acc: 0.7541\n",
            "Epoch 5: val_acc did not improve from 0.72040\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.8721 - acc: 0.7541 - val_loss: 1.0656 - val_acc: 0.7000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 6/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.8281 - acc: 0.7724\n",
            "Epoch 6: val_acc improved from 0.72040 to 0.76650, saving model to /content/saved_models/cifar10_ResNet20v1_model.006.h5\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.8282 - acc: 0.7724 - val_loss: 0.8477 - val_acc: 0.7665 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 7/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.7874 - acc: 0.7902\n",
            "Epoch 7: val_acc did not improve from 0.76650\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.7873 - acc: 0.7902 - val_loss: 1.0369 - val_acc: 0.7008 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 8/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7627 - acc: 0.7993\n",
            "Epoch 8: val_acc improved from 0.76650 to 0.77880, saving model to /content/saved_models/cifar10_ResNet20v1_model.008.h5\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.7628 - acc: 0.7992 - val_loss: 0.8252 - val_acc: 0.7788 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 9/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.8082\n",
            "Epoch 9: val_acc did not improve from 0.77880\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.7358 - acc: 0.8082 - val_loss: 0.9027 - val_acc: 0.7622 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 10/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7167 - acc: 0.8156\n",
            "Epoch 10: val_acc did not improve from 0.77880\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.7165 - acc: 0.8157 - val_loss: 0.8839 - val_acc: 0.7685 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 11/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6990 - acc: 0.8225\n",
            "Epoch 11: val_acc did not improve from 0.77880\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.6989 - acc: 0.8226 - val_loss: 0.8784 - val_acc: 0.7725 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 12/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.8270\n",
            "Epoch 12: val_acc improved from 0.77880 to 0.80300, saving model to /content/saved_models/cifar10_ResNet20v1_model.012.h5\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.6856 - acc: 0.8270 - val_loss: 0.7821 - val_acc: 0.8030 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 13/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6705 - acc: 0.8336\n",
            "Epoch 13: val_acc did not improve from 0.80300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.6704 - acc: 0.8336 - val_loss: 1.0234 - val_acc: 0.7282 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 14/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6626 - acc: 0.8384\n",
            "Epoch 14: val_acc did not improve from 0.80300\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.6624 - acc: 0.8385 - val_loss: 1.1309 - val_acc: 0.7185 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 15/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.6535 - acc: 0.8399\n",
            "Epoch 15: val_acc improved from 0.80300 to 0.81030, saving model to /content/saved_models/cifar10_ResNet20v1_model.015.h5\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.6535 - acc: 0.8399 - val_loss: 0.7637 - val_acc: 0.8103 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 16/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6444 - acc: 0.8449\n",
            "Epoch 16: val_acc improved from 0.81030 to 0.83450, saving model to /content/saved_models/cifar10_ResNet20v1_model.016.h5\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.6444 - acc: 0.8449 - val_loss: 0.6799 - val_acc: 0.8345 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 17/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.6331 - acc: 0.8473\n",
            "Epoch 17: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.6333 - acc: 0.8472 - val_loss: 0.7161 - val_acc: 0.8264 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 18/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.6255 - acc: 0.8512\n",
            "Epoch 18: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.6255 - acc: 0.8512 - val_loss: 0.7984 - val_acc: 0.8037 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 19/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6237 - acc: 0.8533\n",
            "Epoch 19: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.6236 - acc: 0.8533 - val_loss: 0.7247 - val_acc: 0.8203 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 20/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6137 - acc: 0.8550\n",
            "Epoch 20: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 0.6137 - acc: 0.8550 - val_loss: 0.7667 - val_acc: 0.8162 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 21/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.6043 - acc: 0.8618\n",
            "Epoch 21: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.6042 - acc: 0.8618 - val_loss: 0.8737 - val_acc: 0.7773 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 22/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5995 - acc: 0.8621\n",
            "Epoch 22: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5995 - acc: 0.8621 - val_loss: 0.7567 - val_acc: 0.8102 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 23/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5951 - acc: 0.8650\n",
            "Epoch 23: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5951 - acc: 0.8650 - val_loss: 0.7426 - val_acc: 0.8179 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 24/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5945 - acc: 0.8635\n",
            "Epoch 24: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5945 - acc: 0.8635 - val_loss: 0.7931 - val_acc: 0.8039 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 25/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5915 - acc: 0.8653\n",
            "Epoch 25: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5914 - acc: 0.8653 - val_loss: 0.8102 - val_acc: 0.8107 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 26/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5828 - acc: 0.8684\n",
            "Epoch 26: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5828 - acc: 0.8684 - val_loss: 0.7201 - val_acc: 0.8280 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 27/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5792 - acc: 0.8695\n",
            "Epoch 27: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5791 - acc: 0.8695 - val_loss: 0.8394 - val_acc: 0.7930 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 28/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5781 - acc: 0.8698\n",
            "Epoch 28: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5782 - acc: 0.8697 - val_loss: 0.9387 - val_acc: 0.7789 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 29/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5716 - acc: 0.8745\n",
            "Epoch 29: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5715 - acc: 0.8746 - val_loss: 0.7381 - val_acc: 0.8168 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 30/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5715 - acc: 0.8739\n",
            "Epoch 30: val_acc did not improve from 0.83450\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 0.5714 - acc: 0.8739 - val_loss: 0.7866 - val_acc: 0.8079 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 31/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.8744\n",
            "Epoch 31: val_acc improved from 0.83450 to 0.83940, saving model to /content/saved_models/cifar10_ResNet20v1_model.031.h5\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5675 - acc: 0.8743 - val_loss: 0.6989 - val_acc: 0.8394 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 32/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.8751\n",
            "Epoch 32: val_acc did not improve from 0.83940\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 0.5636 - acc: 0.8751 - val_loss: 0.6966 - val_acc: 0.8372 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 33/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5613 - acc: 0.8764\n",
            "Epoch 33: val_acc did not improve from 0.83940\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5611 - acc: 0.8764 - val_loss: 0.7507 - val_acc: 0.8208 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 34/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5595 - acc: 0.8777\n",
            "Epoch 34: val_acc improved from 0.83940 to 0.84650, saving model to /content/saved_models/cifar10_ResNet20v1_model.034.h5\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5595 - acc: 0.8777 - val_loss: 0.6681 - val_acc: 0.8465 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 35/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5564 - acc: 0.8785\n",
            "Epoch 35: val_acc did not improve from 0.84650\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5564 - acc: 0.8785 - val_loss: 0.7450 - val_acc: 0.8207 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 36/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.8800\n",
            "Epoch 36: val_acc did not improve from 0.84650\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5534 - acc: 0.8800 - val_loss: 0.7447 - val_acc: 0.8242 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 37/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.8819\n",
            "Epoch 37: val_acc improved from 0.84650 to 0.85110, saving model to /content/saved_models/cifar10_ResNet20v1_model.037.h5\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 0.5500 - acc: 0.8819 - val_loss: 0.6489 - val_acc: 0.8511 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 38/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5473 - acc: 0.8824\n",
            "Epoch 38: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5473 - acc: 0.8824 - val_loss: 0.7281 - val_acc: 0.8314 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 39/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.8803\n",
            "Epoch 39: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5513 - acc: 0.8803 - val_loss: 0.6985 - val_acc: 0.8420 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 40/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.8824\n",
            "Epoch 40: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5479 - acc: 0.8824 - val_loss: 0.7718 - val_acc: 0.8198 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 41/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5426 - acc: 0.8837\n",
            "Epoch 41: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5426 - acc: 0.8837 - val_loss: 0.7072 - val_acc: 0.8360 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 42/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5361 - acc: 0.8861\n",
            "Epoch 42: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5361 - acc: 0.8861 - val_loss: 0.6680 - val_acc: 0.8494 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 43/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5382 - acc: 0.8866\n",
            "Epoch 43: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5382 - acc: 0.8866 - val_loss: 0.7947 - val_acc: 0.8074 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 44/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.8864\n",
            "Epoch 44: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5340 - acc: 0.8865 - val_loss: 0.6893 - val_acc: 0.8446 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 45/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5358 - acc: 0.8872\n",
            "Epoch 45: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5358 - acc: 0.8872 - val_loss: 0.6994 - val_acc: 0.8407 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 46/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5356 - acc: 0.8885\n",
            "Epoch 46: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5356 - acc: 0.8885 - val_loss: 0.7798 - val_acc: 0.8258 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 47/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.8888\n",
            "Epoch 47: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5314 - acc: 0.8888 - val_loss: 0.6810 - val_acc: 0.8499 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 48/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8895\n",
            "Epoch 48: val_acc did not improve from 0.85110\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 0.5279 - acc: 0.8895 - val_loss: 0.7902 - val_acc: 0.8123 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 49/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.8893\n",
            "Epoch 49: val_acc improved from 0.85110 to 0.86230, saving model to /content/saved_models/cifar10_ResNet20v1_model.049.h5\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5285 - acc: 0.8893 - val_loss: 0.6163 - val_acc: 0.8623 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 50/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5273 - acc: 0.8904\n",
            "Epoch 50: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5273 - acc: 0.8904 - val_loss: 0.6687 - val_acc: 0.8549 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 51/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5224 - acc: 0.8923\n",
            "Epoch 51: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5224 - acc: 0.8923 - val_loss: 0.6239 - val_acc: 0.8599 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 52/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.8907\n",
            "Epoch 52: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5217 - acc: 0.8907 - val_loss: 0.7529 - val_acc: 0.8274 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 53/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.8898\n",
            "Epoch 53: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5248 - acc: 0.8898 - val_loss: 0.7558 - val_acc: 0.8248 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 54/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.8920\n",
            "Epoch 54: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5200 - acc: 0.8920 - val_loss: 0.6438 - val_acc: 0.8543 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 55/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.8925\n",
            "Epoch 55: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5210 - acc: 0.8925 - val_loss: 0.6437 - val_acc: 0.8563 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 56/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.8942\n",
            "Epoch 56: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5171 - acc: 0.8942 - val_loss: 0.7842 - val_acc: 0.8187 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 57/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8949\n",
            "Epoch 57: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 0.5134 - acc: 0.8949 - val_loss: 0.7604 - val_acc: 0.8397 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 58/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.8931\n",
            "Epoch 58: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5169 - acc: 0.8930 - val_loss: 0.8913 - val_acc: 0.8036 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 59/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.8933\n",
            "Epoch 59: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5156 - acc: 0.8933 - val_loss: 0.7430 - val_acc: 0.8355 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 60/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.8940\n",
            "Epoch 60: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5154 - acc: 0.8940 - val_loss: 0.7265 - val_acc: 0.8377 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 61/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.8957\n",
            "Epoch 61: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5150 - acc: 0.8956 - val_loss: 0.7422 - val_acc: 0.8370 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 62/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5114 - acc: 0.8945\n",
            "Epoch 62: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5111 - acc: 0.8946 - val_loss: 0.7637 - val_acc: 0.8286 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 63/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5125 - acc: 0.8952\n",
            "Epoch 63: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5125 - acc: 0.8952 - val_loss: 0.6320 - val_acc: 0.8582 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 64/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8980\n",
            "Epoch 64: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5066 - acc: 0.8980 - val_loss: 0.6922 - val_acc: 0.8496 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 65/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5121 - acc: 0.8947\n",
            "Epoch 65: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5121 - acc: 0.8947 - val_loss: 0.6390 - val_acc: 0.8599 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 66/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.8957\n",
            "Epoch 66: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5079 - acc: 0.8957 - val_loss: 0.6795 - val_acc: 0.8480 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 67/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5050 - acc: 0.8984\n",
            "Epoch 67: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5049 - acc: 0.8984 - val_loss: 0.6668 - val_acc: 0.8475 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 68/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5102 - acc: 0.8962\n",
            "Epoch 68: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5102 - acc: 0.8962 - val_loss: 0.6725 - val_acc: 0.8536 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 69/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.8988\n",
            "Epoch 69: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.5025 - acc: 0.8989 - val_loss: 0.6337 - val_acc: 0.8597 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 70/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.8964\n",
            "Epoch 70: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5091 - acc: 0.8964 - val_loss: 0.6631 - val_acc: 0.8531 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 71/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.8981\n",
            "Epoch 71: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5056 - acc: 0.8980 - val_loss: 0.6713 - val_acc: 0.8528 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 72/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5061 - acc: 0.8979\n",
            "Epoch 72: val_acc did not improve from 0.86230\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5061 - acc: 0.8979 - val_loss: 0.8724 - val_acc: 0.7928 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 73/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.8982\n",
            "Epoch 73: val_acc improved from 0.86230 to 0.86770, saving model to /content/saved_models/cifar10_ResNet20v1_model.073.h5\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.5028 - acc: 0.8982 - val_loss: 0.6117 - val_acc: 0.8677 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 74/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.4988 - acc: 0.8986\n",
            "Epoch 74: val_acc did not improve from 0.86770\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.4988 - acc: 0.8986 - val_loss: 0.7716 - val_acc: 0.8342 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 75/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5032 - acc: 0.8984\n",
            "Epoch 75: val_acc did not improve from 0.86770\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.5032 - acc: 0.8984 - val_loss: 0.7408 - val_acc: 0.8257 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 76/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.5024 - acc: 0.8994\n",
            "Epoch 76: val_acc did not improve from 0.86770\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.5024 - acc: 0.8994 - val_loss: 0.7315 - val_acc: 0.8335 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 77/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.8997\n",
            "Epoch 77: val_acc did not improve from 0.86770\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4957 - acc: 0.8997 - val_loss: 0.8084 - val_acc: 0.8148 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 78/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.9015\n",
            "Epoch 78: val_acc did not improve from 0.86770\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.4966 - acc: 0.9015 - val_loss: 0.6985 - val_acc: 0.8443 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 79/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8996\n",
            "Epoch 79: val_acc did not improve from 0.86770\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 0.4953 - acc: 0.8996 - val_loss: 0.6720 - val_acc: 0.8524 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 80/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.4993 - acc: 0.8981\n",
            "Epoch 80: val_acc did not improve from 0.86770\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.4993 - acc: 0.8980 - val_loss: 0.7509 - val_acc: 0.8292 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 81/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.4960 - acc: 0.9012\n",
            "Epoch 81: val_acc improved from 0.86770 to 0.86870, saving model to /content/saved_models/cifar10_ResNet20v1_model.081.h5\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.4961 - acc: 0.9011 - val_loss: 0.6202 - val_acc: 0.8687 - lr: 0.0010\n",
            "Learning rate:  0.0001\n",
            "Epoch 82/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.4136 - acc: 0.9296\n",
            "Epoch 82: val_acc improved from 0.86870 to 0.90570, saving model to /content/saved_models/cifar10_ResNet20v1_model.082.h5\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.4135 - acc: 0.9297 - val_loss: 0.5067 - val_acc: 0.9057 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 83/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.3774 - acc: 0.9401\n",
            "Epoch 83: val_acc improved from 0.90570 to 0.90590, saving model to /content/saved_models/cifar10_ResNet20v1_model.083.h5\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.3774 - acc: 0.9401 - val_loss: 0.4953 - val_acc: 0.9059 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 84/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.9438\n",
            "Epoch 84: val_acc improved from 0.90590 to 0.91000, saving model to /content/saved_models/cifar10_ResNet20v1_model.084.h5\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.3631 - acc: 0.9438 - val_loss: 0.4883 - val_acc: 0.9100 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 85/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.3523 - acc: 0.9463\n",
            "Epoch 85: val_acc did not improve from 0.91000\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.3523 - acc: 0.9463 - val_loss: 0.4880 - val_acc: 0.9089 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 86/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.9491\n",
            "Epoch 86: val_acc did not improve from 0.91000\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.3416 - acc: 0.9491 - val_loss: 0.4868 - val_acc: 0.9056 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 87/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.3317 - acc: 0.9505\n",
            "Epoch 87: val_acc did not improve from 0.91000\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.3317 - acc: 0.9505 - val_loss: 0.4872 - val_acc: 0.9053 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 88/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.9520\n",
            "Epoch 88: val_acc improved from 0.91000 to 0.91090, saving model to /content/saved_models/cifar10_ResNet20v1_model.088.h5\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.3265 - acc: 0.9520 - val_loss: 0.4683 - val_acc: 0.9109 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 89/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9535\n",
            "Epoch 89: val_acc did not improve from 0.91090\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.3190 - acc: 0.9534 - val_loss: 0.4697 - val_acc: 0.9103 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 90/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.3109 - acc: 0.9560\n",
            "Epoch 90: val_acc did not improve from 0.91090\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.3109 - acc: 0.9560 - val_loss: 0.4734 - val_acc: 0.9085 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 91/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.9556\n",
            "Epoch 91: val_acc did not improve from 0.91090\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.3046 - acc: 0.9556 - val_loss: 0.4726 - val_acc: 0.9090 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 92/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9573\n",
            "Epoch 92: val_acc improved from 0.91090 to 0.91160, saving model to /content/saved_models/cifar10_ResNet20v1_model.092.h5\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2982 - acc: 0.9573 - val_loss: 0.4629 - val_acc: 0.9116 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 93/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9574\n",
            "Epoch 93: val_acc improved from 0.91160 to 0.91300, saving model to /content/saved_models/cifar10_ResNet20v1_model.093.h5\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2963 - acc: 0.9574 - val_loss: 0.4626 - val_acc: 0.9130 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 94/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9597\n",
            "Epoch 94: val_acc did not improve from 0.91300\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.2886 - acc: 0.9597 - val_loss: 0.4643 - val_acc: 0.9110 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 95/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9600\n",
            "Epoch 95: val_acc did not improve from 0.91300\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 0.2852 - acc: 0.9600 - val_loss: 0.4611 - val_acc: 0.9106 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 96/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9608\n",
            "Epoch 96: val_acc did not improve from 0.91300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2787 - acc: 0.9608 - val_loss: 0.4581 - val_acc: 0.9128 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 97/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2736 - acc: 0.9625\n",
            "Epoch 97: val_acc did not improve from 0.91300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2736 - acc: 0.9625 - val_loss: 0.4568 - val_acc: 0.9119 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 98/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9623\n",
            "Epoch 98: val_acc did not improve from 0.91300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2700 - acc: 0.9623 - val_loss: 0.4535 - val_acc: 0.9109 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 99/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2702 - acc: 0.9614\n",
            "Epoch 99: val_acc did not improve from 0.91300\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2702 - acc: 0.9614 - val_loss: 0.4537 - val_acc: 0.9118 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 100/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9652\n",
            "Epoch 100: val_acc improved from 0.91300 to 0.91440, saving model to /content/saved_models/cifar10_ResNet20v1_model.100.h5\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2616 - acc: 0.9653 - val_loss: 0.4499 - val_acc: 0.9144 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 101/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2605 - acc: 0.9637\n",
            "Epoch 101: val_acc did not improve from 0.91440\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2605 - acc: 0.9637 - val_loss: 0.4630 - val_acc: 0.9088 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 102/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9653\n",
            "Epoch 102: val_acc did not improve from 0.91440\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2568 - acc: 0.9653 - val_loss: 0.4658 - val_acc: 0.9099 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 103/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9656\n",
            "Epoch 103: val_acc did not improve from 0.91440\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2550 - acc: 0.9656 - val_loss: 0.4506 - val_acc: 0.9124 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 104/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2512 - acc: 0.9655\n",
            "Epoch 104: val_acc did not improve from 0.91440\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2512 - acc: 0.9655 - val_loss: 0.4557 - val_acc: 0.9089 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 105/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2455 - acc: 0.9668\n",
            "Epoch 105: val_acc improved from 0.91440 to 0.91560, saving model to /content/saved_models/cifar10_ResNet20v1_model.105.h5\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2455 - acc: 0.9668 - val_loss: 0.4546 - val_acc: 0.9156 - lr: 3.1623e-05\n",
            "Learning rate:  0.0001\n",
            "Epoch 106/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2447 - acc: 0.9662\n",
            "Epoch 106: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2447 - acc: 0.9662 - val_loss: 0.4503 - val_acc: 0.9126 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 107/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9672\n",
            "Epoch 107: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2408 - acc: 0.9672 - val_loss: 0.4472 - val_acc: 0.9124 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 108/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9682\n",
            "Epoch 108: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2367 - acc: 0.9682 - val_loss: 0.4567 - val_acc: 0.9110 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 109/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2350 - acc: 0.9683\n",
            "Epoch 109: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2350 - acc: 0.9683 - val_loss: 0.4416 - val_acc: 0.9145 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 110/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9693\n",
            "Epoch 110: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2319 - acc: 0.9693 - val_loss: 0.4558 - val_acc: 0.9128 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 111/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9693\n",
            "Epoch 111: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2314 - acc: 0.9693 - val_loss: 0.4540 - val_acc: 0.9106 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 112/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9693\n",
            "Epoch 112: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2265 - acc: 0.9693 - val_loss: 0.4501 - val_acc: 0.9126 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 113/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9686\n",
            "Epoch 113: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2283 - acc: 0.9686 - val_loss: 0.4438 - val_acc: 0.9144 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 114/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.2218 - acc: 0.9710\n",
            "Epoch 114: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2218 - acc: 0.9710 - val_loss: 0.4508 - val_acc: 0.9106 - lr: 3.1623e-05\n",
            "Learning rate:  0.0001\n",
            "Epoch 115/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9704\n",
            "Epoch 115: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2219 - acc: 0.9704 - val_loss: 0.4496 - val_acc: 0.9110 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 116/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9707\n",
            "Epoch 116: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 0.2193 - acc: 0.9707 - val_loss: 0.4487 - val_acc: 0.9132 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 117/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9718\n",
            "Epoch 117: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2153 - acc: 0.9718 - val_loss: 0.4470 - val_acc: 0.9126 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 118/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9726\n",
            "Epoch 118: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.2129 - acc: 0.9726 - val_loss: 0.4453 - val_acc: 0.9144 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 119/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9724\n",
            "Epoch 119: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.2105 - acc: 0.9725 - val_loss: 0.4597 - val_acc: 0.9129 - lr: 3.1623e-05\n",
            "Learning rate:  0.0001\n",
            "Epoch 120/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9725\n",
            "Epoch 120: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.2092 - acc: 0.9725 - val_loss: 0.4522 - val_acc: 0.9121 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 121/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9730\n",
            "Epoch 121: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.2086 - acc: 0.9730 - val_loss: 0.4586 - val_acc: 0.9096 - lr: 1.0000e-04\n",
            "Learning rate:  1e-05\n",
            "Epoch 122/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9761\n",
            "Epoch 122: val_acc did not improve from 0.91560\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.2000 - acc: 0.9761 - val_loss: 0.4353 - val_acc: 0.9144 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 123/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9783\n",
            "Epoch 123: val_acc improved from 0.91560 to 0.91590, saving model to /content/saved_models/cifar10_ResNet20v1_model.123.h5\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1940 - acc: 0.9783 - val_loss: 0.4345 - val_acc: 0.9159 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 124/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9783\n",
            "Epoch 124: val_acc did not improve from 0.91590\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1932 - acc: 0.9783 - val_loss: 0.4352 - val_acc: 0.9151 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 125/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9779\n",
            "Epoch 125: val_acc did not improve from 0.91590\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1948 - acc: 0.9779 - val_loss: 0.4336 - val_acc: 0.9153 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 126/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1909 - acc: 0.9789\n",
            "Epoch 126: val_acc improved from 0.91590 to 0.91680, saving model to /content/saved_models/cifar10_ResNet20v1_model.126.h5\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1909 - acc: 0.9789 - val_loss: 0.4340 - val_acc: 0.9168 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 127/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1889 - acc: 0.9805\n",
            "Epoch 127: val_acc did not improve from 0.91680\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1889 - acc: 0.9805 - val_loss: 0.4324 - val_acc: 0.9163 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 128/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9802\n",
            "Epoch 128: val_acc improved from 0.91680 to 0.91690, saving model to /content/saved_models/cifar10_ResNet20v1_model.128.h5\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1897 - acc: 0.9801 - val_loss: 0.4330 - val_acc: 0.9169 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 129/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9793\n",
            "Epoch 129: val_acc did not improve from 0.91690\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1900 - acc: 0.9792 - val_loss: 0.4336 - val_acc: 0.9165 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 130/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9796\n",
            "Epoch 130: val_acc improved from 0.91690 to 0.91710, saving model to /content/saved_models/cifar10_ResNet20v1_model.130.h5\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1882 - acc: 0.9796 - val_loss: 0.4338 - val_acc: 0.9171 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 131/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1870 - acc: 0.9803\n",
            "Epoch 131: val_acc did not improve from 0.91710\n",
            "1563/1563 [==============================] - 41s 27ms/step - loss: 0.1870 - acc: 0.9803 - val_loss: 0.4331 - val_acc: 0.9162 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 132/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9806\n",
            "Epoch 132: val_acc did not improve from 0.91710\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 0.1872 - acc: 0.9806 - val_loss: 0.4310 - val_acc: 0.9171 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 133/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9803\n",
            "Epoch 133: val_acc did not improve from 0.91710\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.1864 - acc: 0.9803 - val_loss: 0.4316 - val_acc: 0.9170 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 134/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9794\n",
            "Epoch 134: val_acc improved from 0.91710 to 0.91820, saving model to /content/saved_models/cifar10_ResNet20v1_model.134.h5\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1880 - acc: 0.9794 - val_loss: 0.4315 - val_acc: 0.9182 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 135/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9811\n",
            "Epoch 135: val_acc did not improve from 0.91820\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 0.1850 - acc: 0.9811 - val_loss: 0.4349 - val_acc: 0.9168 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 136/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9810\n",
            "Epoch 136: val_acc did not improve from 0.91820\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1849 - acc: 0.9810 - val_loss: 0.4358 - val_acc: 0.9165 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 137/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9805\n",
            "Epoch 137: val_acc did not improve from 0.91820\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1849 - acc: 0.9805 - val_loss: 0.4334 - val_acc: 0.9173 - lr: 3.1623e-06\n",
            "Learning rate:  1e-05\n",
            "Epoch 138/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9803\n",
            "Epoch 138: val_acc improved from 0.91820 to 0.91890, saving model to /content/saved_models/cifar10_ResNet20v1_model.138.h5\n",
            "1563/1563 [==============================] - 43s 28ms/step - loss: 0.1853 - acc: 0.9803 - val_loss: 0.4321 - val_acc: 0.9189 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 139/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9814\n",
            "Epoch 139: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 0.1831 - acc: 0.9814 - val_loss: 0.4353 - val_acc: 0.9168 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 140/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1844 - acc: 0.9808\n",
            "Epoch 140: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 45s 28ms/step - loss: 0.1844 - acc: 0.9808 - val_loss: 0.4323 - val_acc: 0.9182 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 141/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1831 - acc: 0.9816\n",
            "Epoch 141: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 0.1831 - acc: 0.9816 - val_loss: 0.4343 - val_acc: 0.9181 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 142/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1816 - acc: 0.9814\n",
            "Epoch 142: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1816 - acc: 0.9814 - val_loss: 0.4331 - val_acc: 0.9179 - lr: 3.1623e-06\n",
            "Learning rate:  1e-05\n",
            "Epoch 143/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9804\n",
            "Epoch 143: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1830 - acc: 0.9805 - val_loss: 0.4340 - val_acc: 0.9172 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 144/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9810\n",
            "Epoch 144: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1825 - acc: 0.9810 - val_loss: 0.4343 - val_acc: 0.9168 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 145/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9814\n",
            "Epoch 145: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1827 - acc: 0.9815 - val_loss: 0.4319 - val_acc: 0.9173 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 146/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1798 - acc: 0.9827\n",
            "Epoch 146: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1798 - acc: 0.9827 - val_loss: 0.4325 - val_acc: 0.9176 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 147/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9812\n",
            "Epoch 147: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1811 - acc: 0.9812 - val_loss: 0.4340 - val_acc: 0.9170 - lr: 3.1623e-06\n",
            "Learning rate:  1e-05\n",
            "Epoch 148/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9825\n",
            "Epoch 148: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1802 - acc: 0.9825 - val_loss: 0.4356 - val_acc: 0.9171 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 149/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9809\n",
            "Epoch 149: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1813 - acc: 0.9809 - val_loss: 0.4354 - val_acc: 0.9175 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 150/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9818\n",
            "Epoch 150: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1809 - acc: 0.9818 - val_loss: 0.4339 - val_acc: 0.9177 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 151/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9821\n",
            "Epoch 151: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1793 - acc: 0.9821 - val_loss: 0.4336 - val_acc: 0.9186 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 152/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9817\n",
            "Epoch 152: val_acc did not improve from 0.91890\n",
            "1563/1563 [==============================] - 43s 28ms/step - loss: 0.1796 - acc: 0.9817 - val_loss: 0.4365 - val_acc: 0.9181 - lr: 3.1623e-06\n",
            "Learning rate:  1e-05\n",
            "Epoch 153/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9824\n",
            "Epoch 153: val_acc improved from 0.91890 to 0.91970, saving model to /content/saved_models/cifar10_ResNet20v1_model.153.h5\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1781 - acc: 0.9824 - val_loss: 0.4340 - val_acc: 0.9197 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 154/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1778 - acc: 0.9827\n",
            "Epoch 154: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 0.1778 - acc: 0.9827 - val_loss: 0.4358 - val_acc: 0.9174 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 155/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9820\n",
            "Epoch 155: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1776 - acc: 0.9820 - val_loss: 0.4322 - val_acc: 0.9189 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 156/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9828\n",
            "Epoch 156: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1792 - acc: 0.9828 - val_loss: 0.4355 - val_acc: 0.9180 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 157/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9827\n",
            "Epoch 157: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1772 - acc: 0.9827 - val_loss: 0.4333 - val_acc: 0.9190 - lr: 3.1623e-06\n",
            "Learning rate:  1e-05\n",
            "Epoch 158/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1765 - acc: 0.9829\n",
            "Epoch 158: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1765 - acc: 0.9829 - val_loss: 0.4374 - val_acc: 0.9180 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 159/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1767 - acc: 0.9825\n",
            "Epoch 159: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1767 - acc: 0.9825 - val_loss: 0.4357 - val_acc: 0.9171 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 160/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9826\n",
            "Epoch 160: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1769 - acc: 0.9826 - val_loss: 0.4349 - val_acc: 0.9177 - lr: 1.0000e-05\n",
            "Learning rate:  1e-05\n",
            "Epoch 161/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1762 - acc: 0.9823\n",
            "Epoch 161: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1762 - acc: 0.9823 - val_loss: 0.4364 - val_acc: 0.9175 - lr: 1.0000e-05\n",
            "Learning rate:  1e-06\n",
            "Epoch 162/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9834\n",
            "Epoch 162: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 0.1748 - acc: 0.9834 - val_loss: 0.4352 - val_acc: 0.9181 - lr: 5.0000e-07\n",
            "Learning rate:  1e-06\n",
            "Epoch 163/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9837\n",
            "Epoch 163: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1751 - acc: 0.9837 - val_loss: 0.4364 - val_acc: 0.9177 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 164/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1739 - acc: 0.9837\n",
            "Epoch 164: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1739 - acc: 0.9837 - val_loss: 0.4343 - val_acc: 0.9178 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 165/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1754 - acc: 0.9832\n",
            "Epoch 165: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1754 - acc: 0.9832 - val_loss: 0.4365 - val_acc: 0.9182 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 166/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9824\n",
            "Epoch 166: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1754 - acc: 0.9825 - val_loss: 0.4359 - val_acc: 0.9173 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 167/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1754 - acc: 0.9825\n",
            "Epoch 167: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1754 - acc: 0.9825 - val_loss: 0.4355 - val_acc: 0.9173 - lr: 5.0000e-07\n",
            "Learning rate:  1e-06\n",
            "Epoch 168/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9836\n",
            "Epoch 168: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1743 - acc: 0.9836 - val_loss: 0.4347 - val_acc: 0.9177 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 169/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1754 - acc: 0.9827\n",
            "Epoch 169: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1754 - acc: 0.9827 - val_loss: 0.4352 - val_acc: 0.9180 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 170/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9824\n",
            "Epoch 170: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 0.1753 - acc: 0.9824 - val_loss: 0.4356 - val_acc: 0.9181 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 171/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9831\n",
            "Epoch 171: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1742 - acc: 0.9831 - val_loss: 0.4355 - val_acc: 0.9184 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 172/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9822\n",
            "Epoch 172: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1758 - acc: 0.9822 - val_loss: 0.4344 - val_acc: 0.9177 - lr: 5.0000e-07\n",
            "Learning rate:  1e-06\n",
            "Epoch 173/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9836\n",
            "Epoch 173: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1739 - acc: 0.9836 - val_loss: 0.4356 - val_acc: 0.9181 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 174/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1748 - acc: 0.9834\n",
            "Epoch 174: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1748 - acc: 0.9834 - val_loss: 0.4350 - val_acc: 0.9176 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 175/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9837\n",
            "Epoch 175: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 27ms/step - loss: 0.1733 - acc: 0.9837 - val_loss: 0.4349 - val_acc: 0.9177 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 176/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1743 - acc: 0.9833\n",
            "Epoch 176: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1743 - acc: 0.9833 - val_loss: 0.4353 - val_acc: 0.9178 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 177/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9837\n",
            "Epoch 177: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1743 - acc: 0.9837 - val_loss: 0.4357 - val_acc: 0.9185 - lr: 5.0000e-07\n",
            "Learning rate:  1e-06\n",
            "Epoch 178/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1720 - acc: 0.9840\n",
            "Epoch 178: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1720 - acc: 0.9840 - val_loss: 0.4351 - val_acc: 0.9189 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 179/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9836\n",
            "Epoch 179: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.1736 - acc: 0.9836 - val_loss: 0.4352 - val_acc: 0.9180 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 180/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9832\n",
            "Epoch 180: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1742 - acc: 0.9833 - val_loss: 0.4359 - val_acc: 0.9186 - lr: 1.0000e-06\n",
            "Learning rate:  1e-06\n",
            "Epoch 181/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9835\n",
            "Epoch 181: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1743 - acc: 0.9835 - val_loss: 0.4352 - val_acc: 0.9186 - lr: 1.0000e-06\n",
            "Learning rate:  5e-07\n",
            "Epoch 182/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9839\n",
            "Epoch 182: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 43s 28ms/step - loss: 0.1721 - acc: 0.9839 - val_loss: 0.4349 - val_acc: 0.9187 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 183/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1739 - acc: 0.9835\n",
            "Epoch 183: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1739 - acc: 0.9835 - val_loss: 0.4346 - val_acc: 0.9180 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 184/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9834\n",
            "Epoch 184: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1733 - acc: 0.9833 - val_loss: 0.4350 - val_acc: 0.9184 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 185/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9830\n",
            "Epoch 185: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.1733 - acc: 0.9831 - val_loss: 0.4362 - val_acc: 0.9175 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 186/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9836\n",
            "Epoch 186: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.1737 - acc: 0.9836 - val_loss: 0.4344 - val_acc: 0.9175 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 187/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9844\n",
            "Epoch 187: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1726 - acc: 0.9844 - val_loss: 0.4355 - val_acc: 0.9182 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 188/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9831\n",
            "Epoch 188: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1747 - acc: 0.9831 - val_loss: 0.4353 - val_acc: 0.9179 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 189/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1742 - acc: 0.9827\n",
            "Epoch 189: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1742 - acc: 0.9827 - val_loss: 0.4362 - val_acc: 0.9178 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 190/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1724 - acc: 0.9839\n",
            "Epoch 190: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1724 - acc: 0.9839 - val_loss: 0.4368 - val_acc: 0.9181 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 191/200\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9836\n",
            "Epoch 191: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1732 - acc: 0.9836 - val_loss: 0.4358 - val_acc: 0.9183 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 192/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1748 - acc: 0.9830\n",
            "Epoch 192: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1748 - acc: 0.9830 - val_loss: 0.4350 - val_acc: 0.9183 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 193/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9833\n",
            "Epoch 193: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.1735 - acc: 0.9833 - val_loss: 0.4344 - val_acc: 0.9182 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 194/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1741 - acc: 0.9834\n",
            "Epoch 194: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.1741 - acc: 0.9834 - val_loss: 0.4354 - val_acc: 0.9183 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 195/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1739 - acc: 0.9835\n",
            "Epoch 195: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.1739 - acc: 0.9835 - val_loss: 0.4350 - val_acc: 0.9189 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 196/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9828\n",
            "Epoch 196: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1746 - acc: 0.9828 - val_loss: 0.4331 - val_acc: 0.9190 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 197/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9831\n",
            "Epoch 197: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1742 - acc: 0.9830 - val_loss: 0.4348 - val_acc: 0.9186 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 198/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9837\n",
            "Epoch 198: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1729 - acc: 0.9837 - val_loss: 0.4350 - val_acc: 0.9188 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 199/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9834\n",
            "Epoch 199: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.1722 - acc: 0.9834 - val_loss: 0.4346 - val_acc: 0.9178 - lr: 5.0000e-07\n",
            "Learning rate:  5e-07\n",
            "Epoch 200/200\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1754 - acc: 0.9822\n",
            "Epoch 200: val_acc did not improve from 0.91970\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1754 - acc: 0.9822 - val_loss: 0.4348 - val_acc: 0.9177 - lr: 5.0000e-07\n"
          ]
        }
      ],
      "source": [
        "# run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # this will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False)\n",
        "\n",
        "    # compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    steps_per_epoch =  math.ceil(len(x_train) / batch_size)\n",
        "    # fit the model on the batches generated by datagen.flow().\n",
        "    model.fit(x=datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "              verbose=1,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              steps_per_epoch=steps_per_epoch,\n",
        "              callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T1RdcmNZraON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bc05e1-df5b-4aa1-ff80-ad6c306492a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.4347715377807617\n",
            "Test accuracy: 0.9176999926567078\n"
          ]
        }
      ],
      "source": [
        "# score trained model\n",
        "scores = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "res_net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
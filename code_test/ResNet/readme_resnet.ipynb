{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "$\\mathcal{H}(x)$: Underlying mapping to be fit by a few stacked layers (not necerssarily the entire net) \n",
    "\n",
    "If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., $\\mathcal{H}(x)-x$ (assuming that the input and output are of the same dimensions). So rather than expect stacked layers to approximate H(x), we explicitly let these layers approximate a residual function $\\mathcal{F}(x) := \\mathcal{H}(x) - x$.\n",
    "\n",
    "...(중략)\n",
    "\n",
    "With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.\n",
    "\n",
    "Source: He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of Deep Residual Networks \n",
    "\n",
    "$y_l = h(x_l) + \\mathcal{F}(x_l, W_l)$\n",
    "\n",
    "$x_{l+1} = f(y_l)$\n",
    "\n",
    "- 2가지 가정\n",
    "\n",
    "1) $h(x_l )=x_l$로 identity mapping이다.\n",
    "\n",
    "2) $f$ 역시도 identity mapping이라고 가정한다. $(x_{l+1}≡y_l)$\n",
    "\n",
    "$x_{l+1} = x_l + \\mathcal{F}(x_l, W_l)$\n",
    "\n",
    "$x_{l+2} = x_{l+1} + \\mathcal{F}(x_{l+1}, W_{l+1}) = x_l + \\mathcal{F}(x_l,W_l ) + \\mathcal{F}(x_{l+1}, W_{l+1})$\n",
    "\n",
    "$x_L = x_l + \\sum_{i=l}^{L-1} \\mathcal{F}(x_i, W_i)$\n",
    "\n",
    "- 2가지 특징\n",
    "\n",
    "1) $x_L$은 $x_l$과 residual function의 합으로 만들어 진다.\n",
    "\n",
    "2) $x_L$은 이전의 모든 residual funtion들의 합이다.\n",
    "\n",
    "${\\partial \\varepsilon \\over{\\partial x_l}} = {\\partial \\varepsilon \\over{\\partial x_L}} {\\partial x_L \\over{\\partial x_l}} = {\\partial \\varepsilon \\over{\\partial x_L}} {(1+{\\partial \\over{\\partial x_l}} {\\sum_{i=l}^{L-1} \\mathcal{F}(x_i, W_i)})} $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf26')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e329751ad3fbc78699a5e88abe00d39aafe605dca43acf9894240e004a03697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
